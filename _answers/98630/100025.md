---
date: '2018-06-23T09:40:32.104Z'
user_id: 3973
user_name: 小擦
user_intro: ''
user_avatar: /static/upload/thumb/small50-u-thumb-default.png
upvote: 2
downvote: 0
comments:
    - ''
    - ''
---

我看到这个问题后突发奇想，一瞬感到政治和机器学习有点联系，所以瞎几把写一点。可能没有机器学习背景的人看不懂这个回答，但这个回答的主要想说的就是统计学上的「大数定理」。

我现在感觉到政治制度就类似于机器学习里的model，而人民群众就类似于samples。政治的目的就是learning，也就是让model fits with the samples。中共的贡献不是一个好的model，而是因为它有大量的samples，是一个poor model with large data所以勉强达到了acceptable performance。实际上，在samples足够多的情况下，histogram都能是generative model，都能够给你underlying distribution behinds the samples，那还有什么model不能learn the distribution的？

如果你跟你老板说，「我有14亿sample points而且binary classfication accuracy在60%以上」，你老板肯定会让你尝试更powerful的model，因为这很明显是underfitting，但是如果你跟你老板说，「我已经达到比random guess更高的accuracy了，为什么还要换model？」，那你肯定会被开除，因为这个model就是underfits with the data，它就不适合这些samples。这就是题主问的问题里存在的问题。任何一个model，给它14亿samples，它都能学出点什么东西。

我还看到一个回答，说大多数人对社会的观点看法都是片面的，因为大多数人根本就没有看到社会的全貌，只有少数统治精英才能看到全貌，才能看到问题所在，才能解决问题，以此来抨击民主制度的缺点。然而，这个回答没有意识到，learning的目的就是fit your model with your data，而政治的目的就是更好的服务群众和人民。除非你要学习的不是你samples背后的distribution，那么samples越多，distribution就越准确。如果你只把weight给区区几十万个samples，那是不可能让这个model fit with 全部14亿samples的（也就是跟这几十万个samples overfitting）。每个sample在gradient descent里的优化的确是很local的，但是当你batch size大了以后，就可以让你optimize的function everywhere differentiable，这就是「群众的力量」。如果你的batch size特别小，这是很难converge到任何一个local minimum的。
